{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Late Fusion with Attention-Based Fusion\n",
        "\n",
        "This notebook implements attention-based fusion for multimodal sentiment analysis:\n",
        "- Audio: Wav2Vec2 features\n",
        "- Vision: ResNet50 features\n",
        "- Fusion: Attention mechanism to learn modality importance\n",
        "\n",
        "The attention mechanism computes attention weights Î±i = softmax(Wa Â· [fv, fa])i over modalities,\n",
        "then fuses: ffused = Î£ Î±i * fi, and sends that to a classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using pip from /home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/bin/python...\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "--- All libraries installed! ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 0: SETUP, LIBRARIES & DATA PREPARATION\n",
        "# =============================================================================\n",
        "import sys\n",
        "\n",
        "# Use pip from current kernel's Python executable\n",
        "print(f\"Using pip from {sys.executable}...\")\n",
        "\n",
        "# Install required libraries\n",
        "!{sys.executable} -m pip install matplotlib seaborn tqdm librosa pandas scikit-learn torchvision opencv-python -q\n",
        "!{sys.executable} -m pip install transformers[torch] accelerate -q\n",
        "\n",
        "print(\"--- All libraries installed! ---\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "Load audio and vision data, ensuring consistent video_id matching for fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading audio data...\n",
            "âœ… Loaded 2452 audio files\n",
            "Loading vision data...\n",
            "âœ… Loaded 11428 vision frames\n",
            "âœ… Frames per video: min=4, max=5, mean=4.7\n",
            "âœ… Grouped to 2452 unique videos\n",
            "âœ… Merged dataset: 2452 samples with both audio and vision\n",
            "\n",
            "ðŸ” Data split verification:\n",
            "  Train videos: 1828\n",
            "  Val videos: 312\n",
            "  Test videos: 312\n",
            "  Train-Val overlap: 0\n",
            "  Train-Test overlap: 0\n",
            "  Val-Test overlap: 0\n",
            "  âœ… No data leakage detected.\n",
            "\n",
            "ðŸ“Š Data split:\n",
            "Train: 1828 | Val: 312 | Test: 312\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 1: LOAD DATA (Audio + Vision)\n",
        "# =============================================================================\n",
        "\n",
        "# Paths\n",
        "RAVDESS_AUDIO_PATH = \"/home/siyi/ravdess_audio\"\n",
        "RAVDESS_VIDEO_PATH = \"/home/siyi/ravdess_dataset\"\n",
        "FRAMES_DIR = \"/home/siyi/ravdess_frames\"\n",
        "\n",
        "# Emotion to sentiment mapping\n",
        "emotion_to_sentiment = {\n",
        "    'neutral': 0,   # neutral\n",
        "    'calm': 0,      # neutral\n",
        "    'happy': 1,     # positive\n",
        "    'surprised': 1, # positive\n",
        "    'sad': 2,       # negative\n",
        "    'angry': 2,     # negative\n",
        "    'fearful': 2,   # negative\n",
        "    'disgust': 2    # negative\n",
        "}\n",
        "\n",
        "ravdess_emotion_map = {\n",
        "    '01': 'neutral',\n",
        "    '02': 'calm',\n",
        "    '03': 'happy',\n",
        "    '04': 'sad',\n",
        "    '05': 'angry',\n",
        "    '06': 'fearful',\n",
        "    '07': 'disgust',\n",
        "    '08': 'surprised'\n",
        "}\n",
        "\n",
        "sentiment_list = ['neutral', 'positive', 'negative']\n",
        "\n",
        "# --- Load Audio Data ---\n",
        "print(\"Loading audio data...\")\n",
        "audio_data = []\n",
        "for filename in os.listdir(RAVDESS_AUDIO_PATH):\n",
        "    if filename.endswith('.wav'):\n",
        "        video_id = os.path.splitext(filename)[0]\n",
        "        parts = video_id.split('-')\n",
        "        if len(parts) >= 3:\n",
        "            emotion_code = parts[2]\n",
        "            emotion = ravdess_emotion_map.get(emotion_code)\n",
        "            if emotion:\n",
        "                audio_data.append({\n",
        "                    'video_id': video_id,\n",
        "                    'audio_path': os.path.join(RAVDESS_AUDIO_PATH, filename),\n",
        "                    'emotion': emotion,\n",
        "                    'label': emotion_to_sentiment[emotion]\n",
        "                })\n",
        "\n",
        "audio_df = pd.DataFrame(audio_data)\n",
        "audio_df = audio_df.sort_values('video_id').reset_index(drop=True)\n",
        "print(f\"âœ… Loaded {len(audio_df)} audio files\")\n",
        "\n",
        "# --- Load Vision Data (frames) ---\n",
        "print(\"Loading vision data...\")\n",
        "vision_data = []\n",
        "for filename in os.listdir(FRAMES_DIR):\n",
        "    if filename.endswith('.jpg'):\n",
        "        video_id = filename.split('_frame_')[0]\n",
        "        parts = video_id.split('-')\n",
        "        if len(parts) >= 3:\n",
        "            emotion_code = parts[2]\n",
        "            emotion = ravdess_emotion_map.get(emotion_code)\n",
        "            if emotion:\n",
        "                vision_data.append({\n",
        "                    'video_id': video_id,\n",
        "                    'frame_path': os.path.join(FRAMES_DIR, filename),\n",
        "                    'emotion': emotion,\n",
        "                    'label': emotion_to_sentiment[emotion]\n",
        "                })\n",
        "\n",
        "vision_df = pd.DataFrame(vision_data)\n",
        "print(f\"âœ… Loaded {len(vision_df)} vision frames\")\n",
        "\n",
        "# --- Group vision frames by video_id ---\n",
        "vision_df_grouped = vision_df.groupby('video_id').agg({\n",
        "    'frame_path': list,\n",
        "    'emotion': 'first',\n",
        "    'label': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "frames_per_video = vision_df_grouped['frame_path'].apply(len)\n",
        "print(f\"âœ… Frames per video: min={frames_per_video.min()}, max={frames_per_video.max()}, mean={frames_per_video.mean():.1f}\")\n",
        "print(f\"âœ… Grouped to {len(vision_df_grouped)} unique videos\")\n",
        "\n",
        "# --- Merge audio and vision data by video_id ---\n",
        "fusion_df = audio_df.merge(\n",
        "    vision_df_grouped[['video_id', 'frame_path', 'emotion', 'label']],\n",
        "    on='video_id',\n",
        "    suffixes=('_audio', '_vision'),\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Verify labels match\n",
        "assert (fusion_df['label_audio'] == fusion_df['label_vision']).all(), \"Label mismatch!\"\n",
        "fusion_df['label'] = fusion_df['label_audio']\n",
        "fusion_df = fusion_df.drop(['label_audio', 'label_vision'], axis=1)\n",
        "\n",
        "print(f\"âœ… Merged dataset: {len(fusion_df)} samples with both audio and vision\")\n",
        "\n",
        "# --- Split by actor ID ---\n",
        "fusion_df['actor_id'] = fusion_df['video_id'].apply(lambda x: int(x.split('-')[-1]))\n",
        "\n",
        "train_actor_ids = list(range(1, 19))\n",
        "val_actor_ids = list(range(19, 22))\n",
        "test_actor_ids = list(range(22, 25))\n",
        "\n",
        "train_df = fusion_df[fusion_df['actor_id'].isin(train_actor_ids)].copy().reset_index(drop=True)\n",
        "val_df = fusion_df[fusion_df['actor_id'].isin(val_actor_ids)].copy().reset_index(drop=True)\n",
        "test_df = fusion_df[fusion_df['actor_id'].isin(test_actor_ids)].copy().reset_index(drop=True)\n",
        "\n",
        "# Verify no data leakage\n",
        "train_video_ids = set(train_df['video_id'].unique())\n",
        "val_video_ids = set(val_df['video_id'].unique())\n",
        "test_video_ids = set(test_df['video_id'].unique())\n",
        "\n",
        "print(f\"\\nðŸ” Data split verification:\")\n",
        "print(f\"  Train videos: {len(train_video_ids)}\")\n",
        "print(f\"  Val videos: {len(val_video_ids)}\")\n",
        "print(f\"  Test videos: {len(test_video_ids)}\")\n",
        "print(f\"  Train-Val overlap: {len(train_video_ids & val_video_ids)}\")\n",
        "print(f\"  Train-Test overlap: {len(train_video_ids & test_video_ids)}\")\n",
        "print(f\"  Val-Test overlap: {len(val_video_ids & test_video_ids)}\")\n",
        "\n",
        "if len(train_video_ids & val_video_ids) > 0 or len(train_video_ids & test_video_ids) > 0 or len(val_video_ids & test_video_ids) > 0:\n",
        "    print(\"  âš ï¸  WARNING: Data leakage detected!\")\n",
        "else:\n",
        "    print(\"  âœ… No data leakage detected.\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Data split:\")\n",
        "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pre-trained Models and Extract Features\n",
        "\n",
        "Load the pre-trained audio and vision models, modify them to extract features instead of classifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading audio model (Wav2Vec2)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/lib/python3.13/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded audio model weights\n",
            "Audio feature dimension: 256\n",
            "\n",
            "Loading vision model (ResNet50)...\n",
            "âœ… Loaded vision model weights\n",
            "Vision feature dimension: 2048\n",
            "\n",
            "âœ… Feature dimensions: Audio=256, Vision=2048\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 2: LOAD PRE-TRAINED MODELS (Feature Extractors)\n",
        "# =============================================================================\n",
        "\n",
        "# --- Load Audio Model (Wav2Vec2) ---\n",
        "print(\"Loading audio model (Wav2Vec2)...\")\n",
        "AUDIO_MODEL_PATH = \"/home/siyi/multimodal_final/multimodal-sentiment-analysis/notebooks/best_wav2vec2_two_stage.pth\"\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"\n",
        "TARGET_SAMPLING_RATE = 16000\n",
        "\n",
        "from transformers import AutoFeatureExtractor\n",
        "audio_feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Load model\n",
        "audio_model = AutoModelForAudioClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT, \n",
        "    num_labels=3\n",
        ").to(device)\n",
        "\n",
        "# Load weights\n",
        "if os.path.exists(AUDIO_MODEL_PATH):\n",
        "    audio_model.load_state_dict(torch.load(AUDIO_MODEL_PATH, map_location=device))\n",
        "    print(\"âœ… Loaded audio model weights\")\n",
        "else:\n",
        "    print(\"âš ï¸  Audio model weights not found, using pre-trained only\")\n",
        "\n",
        "# Freeze audio model and use as feature extractor\n",
        "for param in audio_model.parameters():\n",
        "    param.requires_grad = False\n",
        "audio_model.eval()\n",
        "\n",
        "# Get audio feature dimension\n",
        "audio_feature_dim = audio_model.projector.out_features\n",
        "print(f\"Audio feature dimension: {audio_feature_dim}\")\n",
        "\n",
        "# --- Load Vision Model (ResNet50) ---\n",
        "print(\"\\nLoading vision model (ResNet50)...\")\n",
        "VISION_MODEL_PATH = \"/home/siyi/multimodal_final/multimodal-sentiment-analysis/model_weights/resnet50_fer2013_sentiment.pth\"\n",
        "\n",
        "# Load ResNet50\n",
        "vision_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "num_ftrs = vision_model.fc.in_features\n",
        "vision_model.fc = nn.Linear(num_ftrs, 3)  # 3 classes\n",
        "\n",
        "# Move model to device BEFORE loading weights\n",
        "vision_model = vision_model.to(device)\n",
        "\n",
        "# Load weights\n",
        "if os.path.exists(VISION_MODEL_PATH):\n",
        "    vision_model.load_state_dict(torch.load(VISION_MODEL_PATH, map_location=device))\n",
        "    print(\"âœ… Loaded vision model weights\")\n",
        "else:\n",
        "    print(\"âš ï¸  Vision model weights not found, using ImageNet pre-trained only\")\n",
        "\n",
        "# Freeze vision model and use as feature extractor\n",
        "for param in vision_model.parameters():\n",
        "    param.requires_grad = False\n",
        "vision_model.eval()\n",
        "\n",
        "# Create a feature extractor function for ResNet50\n",
        "def extract_resnet_features(model, image):\n",
        "    \"\"\"Extract features from ResNet50 before the fc layer\"\"\"\n",
        "    x = model.conv1(image)\n",
        "    x = model.bn1(x)\n",
        "    x = model.relu(x)\n",
        "    x = model.maxpool(x)\n",
        "    x = model.layer1(x)\n",
        "    x = model.layer2(x)\n",
        "    x = model.layer3(x)\n",
        "    x = model.layer4(x)\n",
        "    x = model.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    return x\n",
        "\n",
        "# Get vision feature dimension\n",
        "vision_feature_dim = num_ftrs  # 2048 for ResNet50\n",
        "print(f\"Vision feature dimension: {vision_feature_dim}\")\n",
        "\n",
        "print(f\"\\nâœ… Feature dimensions: Audio={audio_feature_dim}, Vision={vision_feature_dim}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset for Feature Extraction\n",
        "\n",
        "Create dataset that extracts features from both audio and vision models separately (not concatenated).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating datasets...\n",
            "âœ… Datasets created\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 3: DATASET FOR FEATURE EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "class FusionDataset(Dataset):\n",
        "    \"\"\"Dataset that extracts features from both audio and vision models separately\"\"\"\n",
        "    \n",
        "    def __init__(self, df, audio_model, vision_model, audio_feature_extractor, \n",
        "                 vision_transform, extract_resnet_fn, max_duration_s=5.0):\n",
        "        self.df = df\n",
        "        self.audio_model = audio_model\n",
        "        self.vision_model = vision_model\n",
        "        self.audio_feature_extractor = audio_feature_extractor\n",
        "        self.vision_transform = vision_transform\n",
        "        self.extract_resnet_fn = extract_resnet_fn\n",
        "        self.max_length = int(max_duration_s * TARGET_SAMPLING_RATE)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        audio_path = row['audio_path']\n",
        "        frame_paths = row['frame_path']\n",
        "        label = row['label']\n",
        "        \n",
        "        if isinstance(frame_paths, str):\n",
        "            frame_paths = [frame_paths]\n",
        "        \n",
        "        # Extract audio features\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=None)\n",
        "            if sr != TARGET_SAMPLING_RATE:\n",
        "                audio = librosa.resample(audio, orig_sr=sr, target_sr=TARGET_SAMPLING_RATE)\n",
        "            \n",
        "            audio_tensor = torch.tensor(audio).float()\n",
        "            if len(audio_tensor) > self.max_length:\n",
        "                audio_tensor = audio_tensor[:self.max_length]\n",
        "            else:\n",
        "                pad_len = self.max_length - len(audio_tensor)\n",
        "                audio_tensor = torch.nn.functional.pad(audio_tensor, (0, pad_len))\n",
        "            \n",
        "            inputs = self.audio_feature_extractor(\n",
        "                audio_tensor.numpy(),\n",
        "                sampling_rate=TARGET_SAMPLING_RATE,\n",
        "                max_length=self.max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_attention_mask=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            \n",
        "            input_values = inputs.input_values.to(device)\n",
        "            attention_mask = inputs.attention_mask.to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                audio_outputs = self.audio_model.wav2vec2(input_values, attention_mask=attention_mask)\n",
        "                audio_features = audio_outputs.last_hidden_state.mean(dim=1)\n",
        "                audio_features = self.audio_model.projector(audio_features)\n",
        "                audio_features = audio_features.squeeze(0).cpu()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio {audio_path}: {e}\")\n",
        "            audio_features = torch.zeros(audio_feature_dim)\n",
        "        \n",
        "        # Extract vision features from multiple frames\n",
        "        vision_features_list = []\n",
        "        for frame_path in frame_paths:\n",
        "            try:\n",
        "                image = Image.open(frame_path).convert('RGB')\n",
        "                image = self.vision_transform(image).unsqueeze(0).to(device)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    frame_features = self.extract_resnet_fn(self.vision_model, image)\n",
        "                    vision_features_list.append(frame_features.squeeze(0).cpu())\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing vision frame {frame_path}: {e}\")\n",
        "                vision_features_list.append(torch.zeros(vision_feature_dim))\n",
        "        \n",
        "        # Average pool over frames\n",
        "        if len(vision_features_list) > 0:\n",
        "            vision_features = torch.stack(vision_features_list).mean(dim=0)\n",
        "        else:\n",
        "            vision_features = torch.zeros(vision_feature_dim)\n",
        "        \n",
        "        # Return separate features (not concatenated) for attention fusion\n",
        "        return audio_features, vision_features, label\n",
        "\n",
        "# Vision transform\n",
        "vision_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating datasets...\")\n",
        "train_fusion_dataset = FusionDataset(train_df, audio_model, vision_model, \n",
        "                                     audio_feature_extractor, vision_transform, extract_resnet_features)\n",
        "val_fusion_dataset = FusionDataset(val_df, audio_model, vision_model, \n",
        "                                   audio_feature_extractor, vision_transform, extract_resnet_features)\n",
        "test_fusion_dataset = FusionDataset(test_df, audio_model, vision_model, \n",
        "                                    audio_feature_extractor, vision_transform, extract_resnet_features)\n",
        "\n",
        "print(\"âœ… Datasets created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention-Based Fusion Model\n",
        "\n",
        "Implement attention mechanism for modality fusion:\n",
        "- Compute attention weights: Î±i = softmax(Wa Â· [fv, fa])i\n",
        "- Fuse features: ffused = Î£ Î±i * fi\n",
        "- Classify with MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Attention-based fusion model created\n",
            "   Audio feature dim: 256\n",
            "   Vision feature dim: 2048\n",
            "   Hidden dim: 256\n",
            "   Output dim: 3 (sentiment classes)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 4: ATTENTION-BASED FUSION MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class AttentionFusionModel(nn.Module):\n",
        "    \"\"\"Attention-based fusion model for multimodal sentiment analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, audio_dim, vision_dim, num_classes=3, hidden_dim=256, dropout=0.5):\n",
        "        super(AttentionFusionModel, self).__init__()\n",
        "        \n",
        "        self.audio_dim = audio_dim\n",
        "        self.vision_dim = vision_dim\n",
        "        \n",
        "        # Project both modalities to the same dimension for attention\n",
        "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\n",
        "        \n",
        "        # Attention mechanism\n",
        "        # Compute attention weights: Î±i = softmax(Wa Â· [fv, fa])i\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),  # Wa in the formula\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 2),  # 2 modalities: audio and vision\n",
        "            nn.Softmax(dim=-1)  # Softmax to get attention weights\n",
        "        )\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, audio_features, vision_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            audio_features: [batch_size, audio_dim]\n",
        "            vision_features: [batch_size, vision_dim]\n",
        "        Returns:\n",
        "            logits: [batch_size, num_classes]\n",
        "            attention_weights: [batch_size, 2]\n",
        "        \"\"\"\n",
        "        batch_size = audio_features.size(0)\n",
        "        \n",
        "        # Project to same dimension\n",
        "        audio_proj = self.audio_proj(audio_features)  # [batch, hidden_dim]\n",
        "        vision_proj = self.vision_proj(vision_features)  # [batch, hidden_dim]\n",
        "        \n",
        "        # Concatenate for attention computation: [fv, fa]\n",
        "        concat_features = torch.cat([vision_proj, audio_proj], dim=1)  # [batch, hidden_dim * 2]\n",
        "        \n",
        "        # Compute attention weights: Î±i = softmax(Wa Â· [fv, fa])i\n",
        "        attention_weights = self.attention(concat_features)  # [batch, 2]\n",
        "        \n",
        "        # Apply attention weights: ffused = Î£ Î±i * fi\n",
        "        # attention_weights[:, 0] for vision, attention_weights[:, 1] for audio\n",
        "        vision_weighted = vision_proj * attention_weights[:, 0:1]  # [batch, hidden_dim]\n",
        "        audio_weighted = audio_proj * attention_weights[:, 1:2]  # [batch, hidden_dim]\n",
        "        \n",
        "        # Fused features\n",
        "        fused_features = vision_weighted + audio_weighted  # [batch, hidden_dim]\n",
        "        \n",
        "        # Classify\n",
        "        logits = self.classifier(fused_features)  # [batch, num_classes]\n",
        "        \n",
        "        return logits, attention_weights\n",
        "\n",
        "# Create model\n",
        "fusion_model = AttentionFusionModel(\n",
        "    audio_dim=audio_feature_dim,\n",
        "    vision_dim=vision_feature_dim,\n",
        "    num_classes=3,\n",
        "    hidden_dim=256,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "print(\"âœ… Attention-based fusion model created\")\n",
        "print(f\"   Audio feature dim: {audio_feature_dim}\")\n",
        "print(f\"   Vision feature dim: {vision_feature_dim}\")\n",
        "print(f\"   Hidden dim: 256\")\n",
        "print(f\"   Output dim: 3 (sentiment classes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create DataLoaders\n",
        "\n",
        "Create DataLoaders for training and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… DataLoaders created\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 5: CREATE DATALOADERS\n",
        "# =============================================================================\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle separate audio and vision features\"\"\"\n",
        "    audio_features = torch.stack([item[0] for item in batch])\n",
        "    vision_features = torch.stack([item[1] for item in batch])\n",
        "    labels = torch.tensor([item[2] for item in batch], dtype=torch.long)\n",
        "    return audio_features, vision_features, labels\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_fusion_dataset, batch_size=batch_size, shuffle=True, \n",
        "                          collate_fn=collate_fn, num_workers=0)\n",
        "val_loader = DataLoader(val_fusion_dataset, batch_size=batch_size, shuffle=False, \n",
        "                        collate_fn=collate_fn, num_workers=0)\n",
        "test_loader = DataLoader(test_fusion_dataset, batch_size=batch_size, shuffle=False, \n",
        "                         collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "print(\"âœ… DataLoaders created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup\n",
        "\n",
        "Setup loss function, optimizer, and scheduler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights:\n",
            "  neutral   : 1.7410\n",
            "  positive  : 1.5808\n",
            "  negative  : 0.5573\n",
            "\n",
            "âœ… Training setup complete\n",
            "   Loss: CrossEntropyLoss with class weights\n",
            "   Optimizer: AdamW (lr=1e-3, weight_decay=0.01)\n",
            "   Scheduler: CosineAnnealingLR\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 6: TRAINING SETUP\n",
        "# =============================================================================\n",
        "\n",
        "# Class weights for imbalanced dataset\n",
        "train_labels = train_df['label'].tolist()\n",
        "class_weights_np = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.arange(3),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights_np[0] *= 1.2  # Neutral\n",
        "class_weights_np[1] *= 1.1  # Positive\n",
        "class_weights_np[2] *= 0.9  # Negative class (reduce weight slightly)\n",
        "class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Class weights:\")\n",
        "for sentiment, w in zip(sentiment_list, class_weights.cpu().numpy()):\n",
        "    print(f\"  {sentiment:10s}: {w:.4f}\")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=5e-4, weight_decay=0.05)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "print(\"\\nâœ… Training setup complete\")\n",
        "print(f\"   Loss: CrossEntropyLoss with class weights\")\n",
        "print(f\"   Optimizer: AdamW (lr=1e-3, weight_decay=0.01)\")\n",
        "print(f\"   Scheduler: CosineAnnealingLR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "Train the attention-based fusion model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [02:34<00:00,  2.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 | Train Loss: 0.2001 | Train Acc: 0.9568 | Val Loss: 0.5521 | Val Acc: 0.8237\n",
            "  ðŸ”¥ Saved best model! (Val Acc: 0.8237)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [02:33<00:00,  2.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 | Train Loss: 0.0797 | Train Acc: 0.9907 | Val Loss: 0.5802 | Val Acc: 0.8429\n",
            "  ðŸ”¥ Saved best model! (Val Acc: 0.8429)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [02:32<00:00,  2.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 | Train Loss: 0.0251 | Train Acc: 0.9967 | Val Loss: 0.4436 | Val Acc: 0.8878\n",
            "  ðŸ”¥ Saved best model! (Val Acc: 0.8878)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [02:32<00:00,  2.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 | Train Loss: 0.0251 | Train Acc: 0.9945 | Val Loss: 0.6002 | Val Acc: 0.8590\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [02:32<00:00,  2.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 | Train Loss: 0.0260 | Train Acc: 0.9956 | Val Loss: 0.4849 | Val Acc: 0.8654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [02:32<00:00,  2.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 | Train Loss: 0.0326 | Train Acc: 0.9967 | Val Loss: 0.5129 | Val Acc: 0.8718\n",
            "  Early stopping at epoch 6\n",
            "\n",
            "============================================================\n",
            "Training Completed!\n",
            "============================================================\n",
            "Final Train Accuracy: 0.9967 (99.67%)\n",
            "Final Val Accuracy: 0.8718 (87.18%)\n",
            "Best Val Accuracy: 0.8878 (88.78%)\n",
            "Total Training Time: 1074.47 seconds (17.91 minutes)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 7: TRAINING LOOP\n",
        "# =============================================================================\n",
        "\n",
        "num_epochs = 20\n",
        "patience = 3\n",
        "best_val_acc = 0.0\n",
        "counter = 0\n",
        "\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    fusion_model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for audio_features, vision_features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        audio_features = audio_features.to(device)\n",
        "        vision_features = vision_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = fusion_model(audio_features, vision_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(fusion_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "    \n",
        "    # Validation\n",
        "    fusion_model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for audio_features, vision_features, labels in val_loader:\n",
        "            audio_features = audio_features.to(device)\n",
        "            vision_features = vision_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs, attention_weights = fusion_model(audio_features, vision_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            \n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "    \n",
        "    train_acc = train_correct / train_total\n",
        "    val_acc = val_correct / val_total\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    \n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(fusion_model.state_dict(), \"best_attention_fusion_model.pth\")\n",
        "        counter = 0\n",
        "        print(f\"  ðŸ”¥ Saved best model! (Val Acc: {val_acc:.4f})\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Completed!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final Train Accuracy: {train_accs[-1]:.4f} ({train_accs[-1]*100:.2f}%)\")\n",
        "print(f\"Final Val Accuracy: {val_accs[-1]:.4f} ({val_accs[-1]*100:.2f}%)\")\n",
        "print(f\"Best Val Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "print(f\"Total Training Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluate the attention-based fusion model on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Attention-Based Fusion Model on Test Set ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Attention-Based Fusion Model Classification Report (3 Sentiment Classes) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.91      0.82      0.86        72\n",
            "    positive       0.94      0.88      0.91        72\n",
            "    negative       0.92      0.99      0.95       168\n",
            "\n",
            "    accuracy                           0.92       312\n",
            "   macro avg       0.92      0.89      0.91       312\n",
            "weighted avg       0.92      0.92      0.92       312\n",
            "\n",
            "\n",
            "--- Detailed Metrics ---\n",
            "Accuracy: 0.9231\n",
            "\n",
            "Per-class metrics:\n",
            "  neutral   : Precision=0.9077, Recall=0.8194, F1=0.8613, Support=72\n",
            "  positive  : Precision=0.9403, Recall=0.8750, F1=0.9065, Support=72\n",
            "  negative  : Precision=0.9222, Recall=0.9881, F1=0.9540, Support=168\n",
            "\n",
            "Macro average: Precision=0.9234, Recall=0.8942, F1=0.9073\n",
            "Weighted average: Precision=0.9230, Recall=0.9231, F1=0.9217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Attention Weight Analysis ---\n",
            "Average Vision Attention Weight: 0.4538\n",
            "Average Audio Attention Weight: 0.5462\n",
            "\n",
            "This shows how much the model relies on each modality for classification.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 8: EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"--- Evaluating Attention-Based Fusion Model on Test Set ---\")\n",
        "\n",
        "# Load best model\n",
        "fusion_model.load_state_dict(torch.load(\"best_attention_fusion_model.pth\"))\n",
        "fusion_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_attention_weights = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for audio_features, vision_features, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        audio_features = audio_features.to(device)\n",
        "        vision_features = vision_features.to(device)\n",
        "        \n",
        "        outputs, attention_weights = fusion_model(audio_features, vision_features)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        \n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_attention_weights.append(attention_weights.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average=None, zero_division=0\n",
        ")\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average='macro', zero_division=0\n",
        ")\n",
        "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average='weighted', zero_division=0\n",
        ")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n--- Attention-Based Fusion Model Classification Report (3 Sentiment Classes) ---\")\n",
        "print(classification_report(all_labels, all_preds, target_names=sentiment_list))\n",
        "\n",
        "# Print detailed metrics\n",
        "print(\"\\n--- Detailed Metrics ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"\\nPer-class metrics:\")\n",
        "for i, sentiment in enumerate(sentiment_list):\n",
        "    print(f\"  {sentiment:10s}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}, Support={support[i]}\")\n",
        "print(f\"\\nMacro average: Precision={precision_macro:.4f}, Recall={recall_macro:.4f}, F1={f1_macro:.4f}\")\n",
        "print(f\"Weighted average: Precision={precision_weighted:.4f}, Recall={recall_weighted:.4f}, F1={f1_weighted:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=sentiment_list,\n",
        "    yticklabels=sentiment_list\n",
        ")\n",
        "plt.title('Attention-Based Fusion Model Confusion Matrix - 3 Sentiment Classes')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze attention weights\n",
        "all_attention_weights = np.concatenate(all_attention_weights, axis=0)\n",
        "avg_vision_attention = all_attention_weights[:, 0].mean()\n",
        "avg_audio_attention = all_attention_weights[:, 1].mean()\n",
        "\n",
        "print(\"\\n--- Attention Weight Analysis ---\")\n",
        "print(f\"Average Vision Attention Weight: {avg_vision_attention:.4f}\")\n",
        "print(f\"Average Audio Attention Weight: {avg_audio_attention:.4f}\")\n",
        "print(f\"\\nThis shows how much the model relies on each modality for classification.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Modality Evaluation\n",
        "\n",
        "Evaluate the model when one modality is missing (set to zero).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EVALUATING MISSING MODALITY SCENARIOS\n",
            "======================================================================\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "1. FULL FUSION (Audio + Vision)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating none: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "2. AUDIO-ONLY (Vision features set to zero)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating vision: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "3. VISION-ONLY (Audio features set to zero)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPARISON OF ALL SCENARIOS\n",
            "======================================================================\n",
            "\n",
            "Scenario             Accuracy     Precision    Recall       F1-Score    \n",
            "----------------------------------------------------------------------\n",
            "Full Fusion          0.9231       0.9234       0.8942       0.9073      \n",
            "Audio-Only           0.8173       0.8033       0.8313       0.8078      \n",
            "Vision-Only          0.7564       0.7099       0.6667       0.6809      \n",
            "\n",
            "======================================================================\n",
            "PERFORMANCE DROP ANALYSIS\n",
            "======================================================================\n",
            "Full Fusion Accuracy: 0.9231 (92.31%)\n",
            "Audio-Only Accuracy:  0.8173 (81.73%)\n",
            "  â†’ Performance drop: 0.1058 (10.58 percentage points)\n",
            "Vision-Only Accuracy: 0.7564 (75.64%)\n",
            "  â†’ Performance drop: 0.1667 (16.67 percentage points)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 9: MISSING MODALITY EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EVALUATING MISSING MODALITY SCENARIOS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def evaluate_with_missing_modality(model, loader, missing_modality='none'):\n",
        "    \"\"\"Evaluate model with missing modality\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for audio_features, vision_features, labels in tqdm(loader, desc=f\"Evaluating {missing_modality}\"):\n",
        "            audio_features = audio_features.to(device)\n",
        "            vision_features = vision_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Set missing modality to zero\n",
        "            if missing_modality == 'audio':\n",
        "                audio_features = torch.zeros_like(audio_features)\n",
        "            elif missing_modality == 'vision':\n",
        "                vision_features = torch.zeros_like(vision_features)\n",
        "            \n",
        "            outputs, _ = model(audio_features, vision_features)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average='macro', zero_division=0\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro\n",
        "    }\n",
        "\n",
        "# Evaluate full fusion\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"1. FULL FUSION (Audio + Vision)\")\n",
        "print(\"-\"*70)\n",
        "results_full = evaluate_with_missing_modality(fusion_model, test_loader, 'none')\n",
        "\n",
        "# Evaluate audio-only\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"2. AUDIO-ONLY (Vision features set to zero)\")\n",
        "print(\"-\"*70)\n",
        "results_audio = evaluate_with_missing_modality(fusion_model, test_loader, 'vision')\n",
        "\n",
        "# Evaluate vision-only\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"3. VISION-ONLY (Audio features set to zero)\")\n",
        "print(\"-\"*70)\n",
        "results_vision = evaluate_with_missing_modality(fusion_model, test_loader, 'audio')\n",
        "\n",
        "# Print comparison table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON OF ALL SCENARIOS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Scenario':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Full Fusion':<20} {results_full['accuracy']:<12.4f} {results_full['precision_macro']:<12.4f} {results_full['recall_macro']:<12.4f} {results_full['f1_macro']:<12.4f}\")\n",
        "print(f\"{'Audio-Only':<20} {results_audio['accuracy']:<12.4f} {results_audio['precision_macro']:<12.4f} {results_audio['recall_macro']:<12.4f} {results_audio['f1_macro']:<12.4f}\")\n",
        "print(f\"{'Vision-Only':<20} {results_vision['accuracy']:<12.4f} {results_vision['precision_macro']:<12.4f} {results_vision['recall_macro']:<12.4f} {results_vision['f1_macro']:<12.4f}\")\n",
        "\n",
        "# Calculate performance drop\n",
        "audio_drop = results_full['accuracy'] - results_audio['accuracy']\n",
        "vision_drop = results_full['accuracy'] - results_vision['accuracy']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PERFORMANCE DROP ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Full Fusion Accuracy: {results_full['accuracy']:.4f} ({results_full['accuracy']*100:.2f}%)\")\n",
        "print(f\"Audio-Only Accuracy:  {results_audio['accuracy']:.4f} ({results_audio['accuracy']*100:.2f}%)\")\n",
        "print(f\"  â†’ Performance drop: {audio_drop:.4f} ({audio_drop*100:.2f} percentage points)\")\n",
        "print(f\"Vision-Only Accuracy: {results_vision['accuracy']:.4f} ({results_vision['accuracy']*100:.2f}%)\")\n",
        "print(f\"  â†’ Performance drop: {vision_drop:.4f} ({vision_drop*100:.2f} percentage points)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
