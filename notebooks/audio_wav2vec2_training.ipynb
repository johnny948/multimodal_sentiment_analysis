{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45ryTg-ZLHcI"
   },
   "source": [
    "# wav2vec2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "40e143e654424e8f96a8acc62749b56c",
      "8a2b1e5cd1ed41208a04ce10bd37505d",
      "12ef45e353234cd187b89bd3b4adc885",
      "4934e2a2e84948d993219a887e6f932a",
      "08d5f1ac3e9e43d28bf0f2a1ca3780a9",
      "25ce37ab6aaf4bf6b0dcb920e2d3cb3e",
      "5e10377b33264c4a9ac9727d8b9e8e44",
      "8de1ae82920f4af996346673195773f2",
      "ffb9e0b266b04194bb777818b49b7b1e",
      "fbdc89bc7bca4e7b9af17480f1b51770",
      "9051309263bf4dcab2a84a14085e010f",
      "b63361a7078549919d77d03dbe797aa9",
      "99248a3e649b4b4fb4a6693fc781a3ae",
      "f77ea67f2d644e949477239def77cce3",
      "d5b9e59652574ae39463a62e91acbc6e",
      "ef93f9eb83cb461f90c8b5cb85fab024",
      "82dc818a6f8049b08e9482b672cf3418",
      "e27a9962ac30459c8dae3d495c40ab8c",
      "1622a1fc464742bdaeeec936fb81fd2c",
      "f4f3e6103c844b89b42c71709ee180ba",
      "abc8a5b72f034606b3fe3b9e5d0ef8bb",
      "1b77a27420aa4d7d90af158073b07e0c",
      "dbfe5751c7a843cbac2f760232c268b2",
      "7bcc87bdc1de4542be9816c23ef918d8",
      "789e1107230e4431a3b8fc36ccc0f370",
      "3719dfed7c0840e6bc152595a3c85487",
      "5d5dc262dd2a4ec282a1125b9dbaf63a",
      "ddf902c6d15240f294215ca9ebcec5b9",
      "3c66cc92b6dc443daa79dd82e564f407",
      "aea70cfcdeb94841bc1bc616cbbd1530",
      "678984ff6763443b9e57ad16e65e0c14",
      "5534f8d2d13b4f47b0dfacb3be042b68",
      "099820472ef94622bc930c7dcc27392a",
      "4a05f692345540a7ab688eae2ff4c635",
      "3a4ece5788cb4e4fa02ed2e22c6c7416",
      "2d1043343d8d4d1f8eb0044e1fb0e48c",
      "a7a6f6ff7cac4c1b8dce48b4038af21f",
      "d90a3d5595744e57b456a64dce5a6662",
      "880aab255d3c44798ec400644408b089",
      "f8f99acab78d454b8505aa41c5b2fdb9",
      "ba121eca7b8f47559d54e7a3e02cc32c",
      "8ab02e1b70cf46b6b2c2a38699d64de5",
      "c85639bd3d5240278f96912779017988",
      "4ce71846fdcc4524ae55b33671370e81"
     ]
    },
    "id": "TQJi6vt_uXQM",
    "outputId": "550c4872-0a6d-4be8-ca9b-5f6e44d12327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip from /home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/bin/python...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ ffmpeg is already installed\n",
      "--- All libraries installed! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 0: SETUP, LIBRARIES & DATA PREPARATION\n",
    "# =============================================================================\n",
    "# Install required libraries setup\n",
    "import sys\n",
    "\n",
    "# Use pip from current kernel's Python executable\n",
    "print(f\"Using pip from {sys.executable}...\")\n",
    "\n",
    "# Install first batch of libraries\n",
    "!{sys.executable} -m pip install matplotlib seaborn tqdm librosa pandas scikit-learn -q\n",
    "\n",
    "# Install second batch of libraries\n",
    "!{sys.executable} -m pip install transformers[torch] accelerate -q\n",
    "\n",
    "# Check and install ffmpeg (if needed)\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def check_ffmpeg():\n",
    "    \"\"\"Check if ffmpeg is installed\"\"\"\n",
    "    return shutil.which('ffmpeg') is not None\n",
    "\n",
    "if not check_ffmpeg():\n",
    "    print(\"‚ö†Ô∏è  ffmpeg not found. Attempting to install...\")\n",
    "    try:\n",
    "        # Try to install ffmpeg using apt-get (for Ubuntu/Debian)\n",
    "        result = subprocess.run(\n",
    "            ['sudo', 'apt-get', 'update', '-qq'],\n",
    "            capture_output=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        result = subprocess.run(\n",
    "            ['sudo', 'apt-get', 'install', '-y', 'ffmpeg', '-qq'],\n",
    "            capture_output=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        if check_ffmpeg():\n",
    "            print(\"‚úÖ ffmpeg installed successfully!\")\n",
    "        else:\n",
    "            raise Exception(\"Installation may have failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Automatic installation failed: {e}\")\n",
    "        print(\"\\nüìã Please install ffmpeg manually:\")\n",
    "        print(\"   Ubuntu/Debian: sudo apt-get install ffmpeg\")\n",
    "        print(\"   macOS: brew install ffmpeg\")\n",
    "        print(\"   Or download from: https://ffmpeg.org/download.html\")\n",
    "        print(\"\\n‚ö†Ô∏è  The audio extraction will fail without ffmpeg!\")\n",
    "else:\n",
    "    print(\"‚úÖ ffmpeg is already installed\")\n",
    "\n",
    "print(\"--- All libraries installed! ---\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1854c1a5"
   },
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "This section extracts audio from MP4 files in the RAVDESS dataset. It processes the metadata to create a DataFrame with file paths and sentiment labels, using a sentiment mapping to group related emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4ce1824e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Audio output directory: /home/siyi/ravdess_audio\n",
      "üîÑ Found 2452 MP4 files (only 01- full AV files). Extracting audio...\n",
      "üìÅ 2452 audio files already extracted. Skipping...\n",
      "üîÑ Extracting audio for 0 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting audio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2452/2452 [00:00<00:00, 106956.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total RAVDESS audio files extracted: 2452\n",
      "emotion\n",
      "calm         376\n",
      "happy        376\n",
      "sad          376\n",
      "angry        376\n",
      "fearful      376\n",
      "disgust      192\n",
      "surprised    192\n",
      "neutral      188\n",
      "Name: count, dtype: int64\n",
      "üìÇ Source MP4 path: /home/siyi/ravdess_dataset\n",
      "üìÇ Extracted audio path: /home/siyi/ravdess_audio\n",
      "\n",
      "üéß Example entries:\n",
      "                                            filepath  emotion  \\\n",
      "0  /home/siyi/ravdess_audio/01-01-01-01-01-01-01.wav  neutral   \n",
      "1  /home/siyi/ravdess_audio/01-01-01-01-01-01-02.wav  neutral   \n",
      "2  /home/siyi/ravdess_audio/01-01-01-01-01-01-03.wav  neutral   \n",
      "3  /home/siyi/ravdess_audio/01-01-01-01-01-01-04.wav  neutral   \n",
      "4  /home/siyi/ravdess_audio/01-01-01-01-01-01-05.wav  neutral   \n",
      "\n",
      "               video_id  \n",
      "0  01-01-01-01-01-01-01  \n",
      "1  01-01-01-01-01-01-02  \n",
      "2  01-01-01-01-01-01-03  \n",
      "3  01-01-01-01-01-01-04  \n",
      "4  01-01-01-01-01-01-05  \n",
      "neutral 3.3066875\n",
      "neutral 3.648\n",
      "neutral 3.456\n",
      "neutral 3.3066875\n",
      "neutral 3.6053125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Set RAVDESS Dataset Path (MP4 files) ---\n",
    "# Update this path to point to your RAVDESS dataset directory containing MP4 files\n",
    "RAVDESS_PATH = \"/home/siyi/ravdess_dataset\"  # Change this to your actual path\n",
    "\n",
    "# --- Define emotion code mapping ---\n",
    "ravdess_emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# --- Create directory for extracted audio files ---\n",
    "AUDIO_OUTPUT_DIR = \"/home/siyi/ravdess_audio\"\n",
    "os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Audio output directory: {AUDIO_OUTPUT_DIR}\")\n",
    "\n",
    "# --- Function to extract audio from MP4 file using ffmpeg ---\n",
    "def extract_audio_from_mp4(mp4_path, output_wav_path):\n",
    "    \"\"\"Extract audio from MP4 file and save as WAV using ffmpeg\"\"\"\n",
    "    try:\n",
    "        # Use ffmpeg to extract audio: -i input, -vn (no video), -acodec pcm_s16le (WAV format), -ar 16000 (sample rate)\n",
    "        cmd = [\n",
    "            'ffmpeg',\n",
    "            '-i', mp4_path,\n",
    "            '-vn',  # No video\n",
    "            '-acodec', 'pcm_s16le',  # WAV format\n",
    "            '-ar', '16000',  # Sample rate 16kHz\n",
    "            '-ac', '1',  # Mono channel\n",
    "            '-y',  # Overwrite output file\n",
    "            output_wav_path\n",
    "        ]\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=True\n",
    "        )\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error extracting audio from {mp4_path}: {e.stderr.decode()}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio from {mp4_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Check if ffmpeg is available before starting extraction ---\n",
    "import shutil\n",
    "if not shutil.which('ffmpeg'):\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå ffmpeg is not installed!\\n\"\n",
    "        \"Please install ffmpeg first:\\n\"\n",
    "        \"  Ubuntu/Debian: sudo apt-get install ffmpeg\\n\"\n",
    "        \"  macOS: brew install ffmpeg\\n\"\n",
    "        \"  Or download from: https://ffmpeg.org/download.html\\n\"\n",
    "        \"\\nAfter installing, restart the kernel and run this cell again.\"\n",
    "    )\n",
    "\n",
    "# --- Collect all .mp4 files first ---\n",
    "# Now we process ALL folders including Audio_Speech_Actors_XX\n",
    "# But only extract files that start with \"01-\" (full AV), skip \"02-\" (video only)\n",
    "mp4_files = []\n",
    "for dirpath, _, filenames in os.walk(RAVDESS_PATH):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.mp4'):\n",
    "            # Only process files that start with \"01-\" (full AV)\n",
    "            # Skip files starting with \"02-\" (video only, no useful audio)\n",
    "            if filename.startswith('01-'):\n",
    "                mp4_files.append((dirpath, filename))\n",
    "\n",
    "print(f\"üîÑ Found {len(mp4_files)} MP4 files (only 01- full AV files). Extracting audio...\")\n",
    "\n",
    "# --- Count how many audio files already exist ---\n",
    "already_extracted = 0\n",
    "for dirpath, filename in mp4_files:\n",
    "    wav_filename = filename.replace('.mp4', '.wav')\n",
    "    wav_path = os.path.join(AUDIO_OUTPUT_DIR, wav_filename)\n",
    "    if os.path.exists(wav_path):\n",
    "        already_extracted += 1\n",
    "\n",
    "if already_extracted > 0:\n",
    "    print(f\"üìÅ {already_extracted} audio files already extracted. Skipping...\")\n",
    "    print(f\"üîÑ Extracting audio for {len(mp4_files) - already_extracted} files...\")\n",
    "\n",
    "# --- Extract audio from MP4 files with progress bar ---\n",
    "ravdess_data = []\n",
    "for dirpath, filename in tqdm(mp4_files, desc=\"Extracting audio\"):\n",
    "    # Parse emotion code from filename (format: XX-XX-XX-XX-XX-XX-XX-XX.mp4)\n",
    "    parts = filename.replace('.mp4', '').split('-')\n",
    "    if len(parts) >= 3:\n",
    "        emotion_code = parts[2]\n",
    "        emotion = ravdess_emotion_map.get(emotion_code)\n",
    "        \n",
    "        if emotion:\n",
    "            mp4_path = os.path.join(dirpath, filename)\n",
    "            # Create output WAV filename\n",
    "            wav_filename = filename.replace('.mp4', '.wav')\n",
    "            wav_path = os.path.join(AUDIO_OUTPUT_DIR, wav_filename)\n",
    "            \n",
    "            # Check if audio file already exists\n",
    "            if os.path.exists(wav_path):\n",
    "                # Skip extraction, just add to data\n",
    "                ravdess_data.append({\n",
    "                    \"filepath\": wav_path,\n",
    "                    \"emotion\": emotion\n",
    "                })\n",
    "            else:\n",
    "                # Extract audio from MP4\n",
    "                if extract_audio_from_mp4(mp4_path, wav_path):\n",
    "                    ravdess_data.append({\n",
    "                        \"filepath\": wav_path,\n",
    "                        \"emotion\": emotion\n",
    "                    })\n",
    "\n",
    "# --- Create DataFrame ---\n",
    "ravdess_df = pd.DataFrame(ravdess_data)\n",
    "\n",
    "# --- Extract video ID from filepath (e.g., \"01-01-01-01-01-01-01\" from \"path/to/01-01-01-01-01-01-01.wav\")\n",
    "ravdess_df['video_id'] = ravdess_df['filepath'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "# --- Sort by video_id to ensure consistent ordering (same as vision model) ---\n",
    "ravdess_df = ravdess_df.sort_values('video_id').reset_index(drop=True)\n",
    "\n",
    "# --- Print dataset summary ---\n",
    "print(f\"\\n‚úÖ Total RAVDESS audio files extracted: {len(ravdess_df)}\")\n",
    "print(ravdess_df['emotion'].value_counts())\n",
    "print(f\"üìÇ Source MP4 path: {RAVDESS_PATH}\")\n",
    "print(f\"üìÇ Extracted audio path: {AUDIO_OUTPUT_DIR}\")\n",
    "print(\"\\nüéß Example entries:\")\n",
    "print(ravdess_df.head())\n",
    "\n",
    "# --- Check audio file durations ---\n",
    "import soundfile as sf\n",
    "\n",
    "for i in range(5):\n",
    "    data, sr = sf.read(ravdess_df['filepath'][i])\n",
    "    print(ravdess_df['emotion'][i], len(data)/sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86de5576"
   },
   "source": [
    "## Wav2Vec2 Preparation and Dataset Class\n",
    "\n",
    "This section loads the pre-trained Wav2Vec 2.0 feature extractor and defines the `AudioDataset` class. This class handles loading the audio files, resampling them to the target sampling rate, and processing them using the Wav2Vec 2.0 feature extractor. It also includes a `collate_fn` to handle potential errors during audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded feature_extractor!\n",
      "\n",
      "üìä Data split by actor ID:\n",
      "Train: 1828 | Val: 312 | Test: 312\n",
      "Train emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "Val emotions: ['neutral' 'calm' 'happy' 'sad' 'angry' 'fearful' 'disgust' 'surprised']\n",
      "\n",
      "üìä Sentiment Class Mapping (8 emotions -> 3 sentiments):\n",
      "  0 = neutral (from: neutral, calm)\n",
      "  1 = positive (from: happy, surprised)\n",
      "  2 = negative (from: sad, angry, fearful, disgust)\n",
      "\n",
      "Train sentiment distribution:\n",
      "  0 (neutral): 420\n",
      "  1 (positive): 424\n",
      "  2 (negative): 984\n",
      "\n",
      "Val sentiment distribution:\n",
      "  0 (neutral): 72\n",
      "  1 (positive): 72\n",
      "  2 (negative): 168\n",
      "\n",
      "Test sentiment distribution:\n",
      "  0 (neutral): 72\n",
      "  1 (positive): 72\n",
      "  2 (negative): 168\n",
      "\n",
      "‚úÖ Audio Dataset + Dataloaders created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/lib/python3.13/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: WAV2VEC2 PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "import torchaudio\n",
    "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_CHECKPOINT)\n",
    "print(\"\\nSuccessfully loaded feature_extractor!\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Correct AudioDataset (includes valid augmentation + attention_mask)\n",
    "# ===============================\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, feature_extractor, max_duration_s=5.0, augment=False):\n",
    "        self.filepaths = df['filepath'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = int(max_duration_s * TARGET_SAMPLING_RATE)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.filepaths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # --- Load audio ---\n",
    "            audio, sr = librosa.load(filepath, sr=None)\n",
    "\n",
    "            # Simple silence removal (important for emotion recognition!)\n",
    "            # top_db=30 means remove sounds below 30dB from peak\n",
    "            audio, _ = librosa.effects.trim(audio, top_db=30)\n",
    "\n",
    "            # --- Resample ---\n",
    "            if sr != TARGET_SAMPLING_RATE:\n",
    "                audio = librosa.resample(audio, orig_sr=sr, target_sr=TARGET_SAMPLING_RATE)\n",
    "\n",
    "            audio_tensor = torch.tensor(audio).float()\n",
    "\n",
    "            if self.augment:\n",
    "\n",
    "                # 1) Add slight noise\n",
    "                if np.random.rand() < 0.5:\n",
    "                    noise = 0.003 * torch.randn_like(audio_tensor)\n",
    "                    audio_tensor = audio_tensor + noise\n",
    "\n",
    "                # 2) Random volume\n",
    "                if np.random.rand() < 0.5:\n",
    "                    gain = np.random.uniform(0.9, 1.1)\n",
    "                    audio_tensor = audio_tensor * gain\n",
    "\n",
    "                # 3) Slight time stretching\n",
    "                if np.random.rand() < 0.1:\n",
    "                    rate = np.random.uniform(0.9, 1.1)\n",
    "                    new_sr = int(TARGET_SAMPLING_RATE * rate)\n",
    "                    audio_tensor = torchaudio.functional.resample(\n",
    "                        audio_tensor.unsqueeze(0),\n",
    "                        orig_freq=TARGET_SAMPLING_RATE,\n",
    "                        new_freq=new_sr\n",
    "                    ).squeeze(0)\n",
    "            if len(audio_tensor) > self.max_length:\n",
    "                audio_tensor = audio_tensor[:self.max_length]   # Truncate\n",
    "            else:\n",
    "                pad_len = self.max_length - len(audio_tensor)\n",
    "                audio_tensor = torch.nn.functional.pad(audio_tensor, (0, pad_len))  # Pad\n",
    "            # --- wav2vec2 feature extractor ---\n",
    "            inputs = self.feature_extractor(\n",
    "                audio_tensor.numpy(),\n",
    "                sampling_rate=TARGET_SAMPLING_RATE,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            input_values = inputs.input_values.squeeze(0)\n",
    "            attention_mask = inputs.attention_mask.squeeze(0)\n",
    "\n",
    "            return input_values, attention_mask, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Label Encoding + Actor Split (8 emotions -> 3 sentiments)\n",
    "# =============================================================================\n",
    "\n",
    "# Map 8 emotions to 3 sentiment classes: 0=neutral, 1=positive, 2=negative\n",
    "# neutral -> neutral (0)\n",
    "# calm -> neutral (0)\n",
    "# happy -> positive (1)\n",
    "# surprised -> positive (1)\n",
    "# sad -> negative (2)\n",
    "# angry -> negative (2)\n",
    "# fearful -> negative (2)\n",
    "# disgust -> negative (2)\n",
    "emotion_to_sentiment = {\n",
    "    'neutral': 0,   # neutral\n",
    "    'calm': 0,      # neutral\n",
    "    'happy': 1,     # positive\n",
    "    'surprised': 1, # positive\n",
    "    'sad': 2,       # negative\n",
    "    'angry': 2,     # negative\n",
    "    'fearful': 2,   # negative\n",
    "    'disgust': 2    # negative\n",
    "}\n",
    "\n",
    "# Fixed RAVDESS emotion order (for display)\n",
    "emotion_list = ['neutral','calm','happy','sad','angry','fearful','disgust','surprised']\n",
    "# 3 sentiment classes\n",
    "sentiment_list = ['neutral', 'positive', 'negative']\n",
    "\n",
    "# Map label (8 classes -> 3 classes)\n",
    "ravdess_df['label'] = ravdess_df['emotion'].apply(lambda x: emotion_to_sentiment[x])\n",
    "\n",
    "\n",
    "# Parse actor id from video_id\n",
    "ravdess_df['actor_id'] = ravdess_df['video_id'].apply(lambda x: int(x.split('-')[-1]))\n",
    "\n",
    "# Actor-based split (strictly speaker-independent)\n",
    "train_actor_ids = list(range(1, 19))\n",
    "val_actor_ids = list(range(19, 22))\n",
    "test_actor_ids = list(range(22, 25))\n",
    "\n",
    "X_train_df = ravdess_df[ravdess_df['actor_id'].isin(train_actor_ids)].copy()\n",
    "X_val_df   = ravdess_df[ravdess_df['actor_id'].isin(val_actor_ids)].copy()\n",
    "X_test_df  = ravdess_df[ravdess_df['actor_id'].isin(test_actor_ids)].copy()\n",
    "\n",
    "X_train_df = X_train_df.reset_index(drop=True)\n",
    "X_val_df   = X_val_df.reset_index(drop=True)\n",
    "X_test_df  = X_test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nüìä Data split by actor ID:\")\n",
    "print(f\"Train: {len(X_train_df)} | Val: {len(X_val_df)} | Test: {len(X_test_df)}\")\n",
    "print(f\"Train emotions: {X_train_df['emotion'].unique()}\")\n",
    "print(f\"Val emotions: {X_val_df['emotion'].unique()}\")\n",
    "\n",
    "print(\"\\nüìä Sentiment Class Mapping (8 emotions -> 3 sentiments):\")\n",
    "print(\"  0 = neutral (from: neutral, calm)\")\n",
    "print(\"  1 = positive (from: happy, surprised)\")\n",
    "print(\"  2 = negative (from: sad, angry, fearful, disgust)\")\n",
    "\n",
    "print(\"\\nTrain sentiment distribution:\")\n",
    "train_sentiment_dist = X_train_df['label'].value_counts().sort_index()\n",
    "for label, count in train_sentiment_dist.items():\n",
    "    sentiment_name = sentiment_list[label]\n",
    "    print(f\"  {label} ({sentiment_name}): {count}\")\n",
    "\n",
    "print(\"\\nVal sentiment distribution:\")\n",
    "val_sentiment_dist = X_val_df['label'].value_counts().sort_index()\n",
    "for label, count in val_sentiment_dist.items():\n",
    "    sentiment_name = sentiment_list[label]\n",
    "    print(f\"  {label} ({sentiment_name}): {count}\")\n",
    "\n",
    "print(\"\\nTest sentiment distribution:\")\n",
    "test_sentiment_dist = X_test_df['label'].value_counts().sort_index()\n",
    "for label, count in test_sentiment_dist.items():\n",
    "    sentiment_name = sentiment_list[label]\n",
    "    print(f\"  {label} ({sentiment_name}): {count}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset + DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "train_dataset = AudioDataset(X_train_df, feature_extractor, augment=True)\n",
    "val_dataset   = AudioDataset(X_val_df,   feature_extractor, augment=False)\n",
    "test_dataset  = AudioDataset(X_test_df,  feature_extractor, augment=False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"\\n‚úÖ Audio Dataset + Dataloaders created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31cb70d3"
   },
   "source": [
    "## Model and Training Setup\n",
    "\n",
    "This section loads the pre-trained Wav2Vec 2.0 model for audio classification and sets up the loss function and optimizer. It also calculates class weights to handle the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "326b7435"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siyi/multimodal_final/multimodal-sentiment-analysis/venv/lib/python3.13/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:\n",
      "neutral    : 1.7410\n",
      "positive   : 1.5808\n",
      "negative   : 0.5573\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: MODEL & TRAINING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "NUM_CLASSES = len(sentiment_list)   # 3 sentiment classes: neutral, positive, negative\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "# Calculate class weights to handle imbalanced dataset\n",
    "class_weights_np = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.arange(NUM_CLASSES), \n",
    "    y=X_train_df['label']\n",
    ")\n",
    "class_weights_np[0] *= 1.2  # Neutral\n",
    "class_weights_np[1] *= 1.1  # Positive\n",
    "class_weights_np[2] *= 0.9  # Negative class (reduce weight slightly)\n",
    "class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class weights:\")\n",
    "for sentiment, w in zip(sentiment_list, class_weights.cpu().numpy()):\n",
    "    print(f\"{sentiment:10s} : {w:.4f}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aaf7e60"
   },
   "source": [
    "## Two-Stage Fine-Tuning\n",
    "\n",
    "This section implements a two-stage fine-tuning process for the Wav2Vec 2.0 model. In Stage 1, only the classification head is trained with the base model frozen. In Stage 2, all layers are unfrozen and the entire model is fine-tuned with a lower learning rate and a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bbccd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1: Freezing base model and training classifier head ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:27<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 1 | Train Loss: 1.0858 | Val Loss: 1.1031 | Val Acc: 0.2372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:11<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 2 | Train Loss: 1.0610 | Val Loss: 1.0824 | Val Acc: 0.3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:21<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 3 | Train Loss: 1.0541 | Val Loss: 1.0719 | Val Acc: 0.4840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:13<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 4 | Train Loss: 1.0423 | Val Loss: 1.0649 | Val Acc: 0.4904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:02<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 - Epoch 5 | Train Loss: 1.0386 | Val Loss: 1.0592 | Val Acc: 0.4936\n",
      "\n",
      "--- STAGE 2: Unfreezing all layers and fine-tuning the entire model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:25<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 1 | Train Loss: 0.9789 | Val Loss: 0.9242 | Val Acc: 0.5769\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 2/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:25<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 2 | Train Loss: 0.7665 | Val Loss: 0.8276 | Val Acc: 0.6346\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 3/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:18<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 3 | Train Loss: 0.6918 | Val Loss: 0.7488 | Val Acc: 0.7019\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 4/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:36<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 4 | Train Loss: 0.5547 | Val Loss: 0.6839 | Val Acc: 0.7212\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 5/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:31<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 5 | Train Loss: 0.4732 | Val Loss: 0.7924 | Val Acc: 0.7244\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 6/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:35<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 6 | Train Loss: 0.3930 | Val Loss: 0.9983 | Val Acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 7/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 7 | Train Loss: 0.3422 | Val Loss: 0.9705 | Val Acc: 0.7853\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 8/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:15<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 8 | Train Loss: 0.3411 | Val Loss: 1.1055 | Val Acc: 0.7949\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 9/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:47<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 9 | Train Loss: 0.2958 | Val Loss: 1.3200 | Val Acc: 0.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 10/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:04<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 10 | Train Loss: 0.2197 | Val Loss: 1.2504 | Val Acc: 0.7981\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 11/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:28<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 11 | Train Loss: 0.1958 | Val Loss: 1.3258 | Val Acc: 0.8013\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 12/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:42<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 12 | Train Loss: 0.2081 | Val Loss: 1.2575 | Val Acc: 0.8109\n",
      "üî• Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 13/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:48<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 13 | Train Loss: 0.2182 | Val Loss: 1.2887 | Val Acc: 0.8077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 14/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:45<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 14 | Train Loss: 0.1516 | Val Loss: 1.3460 | Val Acc: 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 15/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [03:08<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 - Epoch 15 | Train Loss: 0.1476 | Val Loss: 1.3175 | Val Acc: 0.8109\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: TWO-STAGE FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# --- STAGE 1: Train only the classification head ---\n",
    "print(\"\\n--- STAGE 1: Freezing base model and training classifier head ---\")\n",
    "\n",
    "# Freeze wav2vec2 backbone\n",
    "for param in model.wav2vec2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train classifier head\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=3e-4)\n",
    "STAGE1_EPOCHS = 5\n",
    "\n",
    "for epoch in range(STAGE1_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, masks, labels in tqdm(train_loader, desc=f\"Stage 1 - Epoch {epoch+1}/{STAGE1_EPOCHS}\"):\n",
    "        if inputs is None:\n",
    "            continue\n",
    "\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks)\n",
    "\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks, labels in val_loader:\n",
    "            if inputs is None:\n",
    "                continue\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Stage 1 - Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# --- STAGE 2: Unfreeze and fine-tune entire model ---\n",
    "print(\"\\n--- STAGE 2: Unfreezing all layers and fine-tuning the entire model ---\")\n",
    "\n",
    "# Unfreeze everything\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": model.wav2vec2.feature_extractor.parameters(), \"lr\": 0.0}, # Completely freeze bottom CNN feature extractor (usually no fine-tuning needed)\n",
    "        {\"params\": model.wav2vec2.encoder.parameters(), \"lr\": 1e-5},          # Fine-tune Transformer layers with small LR\n",
    "        {\"params\": model.wav2vec2.feature_projection.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 5e-4}                 # Give classifier head larger LR\n",
    "    ],\n",
    "    weight_decay=0.01 # Add weight_decay to prevent overfitting\n",
    ")\n",
    "\n",
    "STAGE2_EPOCHS = 15\n",
    "total_steps = STAGE2_EPOCHS * len(train_loader)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(STAGE2_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, masks, labels in tqdm(train_loader, desc=f\"Stage 2 - Epoch {epoch+1}/{STAGE2_EPOCHS}\"):\n",
    "        if inputs is None:\n",
    "            continue\n",
    "\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks, labels in val_loader:\n",
    "            if inputs is None:\n",
    "                continue\n",
    "\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Stage 2 - Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best (based on validation accuracy)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_wav2vec2_two_stage.pth\")\n",
    "        print(\"üî• Saved best model!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74a029a7"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This section evaluates the final fine-tuned Wav2Vec 2.0 model on the hold-out test set. It loads the best model state dictionary, performs inference, and then displays the classification report and confusion matrix to assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a156c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the final fine-tuned Wav2Vec2 model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Wav2Vec2 Classification Report (2-Stage Training) - 3 Sentiment Classes ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.56      0.97      0.71        72\n",
      "    positive       0.91      0.68      0.78        72\n",
      "    negative       0.95      0.76      0.84       168\n",
      "\n",
      "    accuracy                           0.79       312\n",
      "   macro avg       0.81      0.80      0.78       312\n",
      "weighted avg       0.85      0.79      0.80       312\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: EVALUATE THE FINAL MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n--- Evaluating the final fine-tuned Wav2Vec2 model ---\")\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('best_wav2vec2_two_stage.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, masks, labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        if inputs is None:\n",
    "            continue\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        masks  = masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs, attention_mask=masks)\n",
    "        predicted = outputs.logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ---- Classification Report ----\n",
    "print(\"\\n--- Final Wav2Vec2 Classification Report (2-Stage Training) - 3 Sentiment Classes ---\")\n",
    "print(classification_report(all_labels, all_preds, target_names=sentiment_list))\n",
    "\n",
    "# ---- Confusion Matrix ----\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=sentiment_list,\n",
    "    yticklabels=sentiment_list\n",
    ")\n",
    "#plt.title('Final Wav2Vec2 Confusion Matrix (2-Stage) - 3 Sentiment Classes')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08d5f1ac3e9e43d28bf0f2a1ca3780a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "099820472ef94622bc930c7dcc27392a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12ef45e353234cd187b89bd3b4adc885": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8de1ae82920f4af996346673195773f2",
      "max": 159,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffb9e0b266b04194bb777818b49b7b1e",
      "value": 159
     }
    },
    "1622a1fc464742bdaeeec936fb81fd2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "1b77a27420aa4d7d90af158073b07e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25ce37ab6aaf4bf6b0dcb920e2d3cb3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d1043343d8d4d1f8eb0044e1fb0e48c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba121eca7b8f47559d54e7a3e02cc32c",
      "max": 380204696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ab02e1b70cf46b6b2c2a38699d64de5",
      "value": 380204696
     }
    },
    "3719dfed7c0840e6bc152595a3c85487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5534f8d2d13b4f47b0dfacb3be042b68",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_099820472ef94622bc930c7dcc27392a",
      "value": "‚Äá380M/380M‚Äá[00:06&lt;00:00,‚Äá34.9MB/s]"
     }
    },
    "3a4ece5788cb4e4fa02ed2e22c6c7416": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_880aab255d3c44798ec400644408b089",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f8f99acab78d454b8505aa41c5b2fdb9",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "3c66cc92b6dc443daa79dd82e564f407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40e143e654424e8f96a8acc62749b56c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a2b1e5cd1ed41208a04ce10bd37505d",
       "IPY_MODEL_12ef45e353234cd187b89bd3b4adc885",
       "IPY_MODEL_4934e2a2e84948d993219a887e6f932a"
      ],
      "layout": "IPY_MODEL_08d5f1ac3e9e43d28bf0f2a1ca3780a9"
     }
    },
    "4934e2a2e84948d993219a887e6f932a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbdc89bc7bca4e7b9af17480f1b51770",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9051309263bf4dcab2a84a14085e010f",
      "value": "‚Äá159/159‚Äá[00:00&lt;00:00,‚Äá4.26kB/s]"
     }
    },
    "4a05f692345540a7ab688eae2ff4c635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a4ece5788cb4e4fa02ed2e22c6c7416",
       "IPY_MODEL_2d1043343d8d4d1f8eb0044e1fb0e48c",
       "IPY_MODEL_a7a6f6ff7cac4c1b8dce48b4038af21f"
      ],
      "layout": "IPY_MODEL_d90a3d5595744e57b456a64dce5a6662"
     }
    },
    "4ce71846fdcc4524ae55b33671370e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5534f8d2d13b4f47b0dfacb3be042b68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d5dc262dd2a4ec282a1125b9dbaf63a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e10377b33264c4a9ac9727d8b9e8e44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "678984ff6763443b9e57ad16e65e0c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "789e1107230e4431a3b8fc36ccc0f370": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aea70cfcdeb94841bc1bc616cbbd1530",
      "max": 380267417,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_678984ff6763443b9e57ad16e65e0c14",
      "value": 380267417
     }
    },
    "7bcc87bdc1de4542be9816c23ef918d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddf902c6d15240f294215ca9ebcec5b9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3c66cc92b6dc443daa79dd82e564f407",
      "value": "pytorch_model.bin:‚Äá100%"
     }
    },
    "82dc818a6f8049b08e9482b672cf3418": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "880aab255d3c44798ec400644408b089": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a2b1e5cd1ed41208a04ce10bd37505d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25ce37ab6aaf4bf6b0dcb920e2d3cb3e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5e10377b33264c4a9ac9727d8b9e8e44",
      "value": "preprocessor_config.json:‚Äá100%"
     }
    },
    "8ab02e1b70cf46b6b2c2a38699d64de5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8de1ae82920f4af996346673195773f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9051309263bf4dcab2a84a14085e010f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99248a3e649b4b4fb4a6693fc781a3ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82dc818a6f8049b08e9482b672cf3418",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e27a9962ac30459c8dae3d495c40ab8c",
      "value": "config.json:‚Äá"
     }
    },
    "a7a6f6ff7cac4c1b8dce48b4038af21f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c85639bd3d5240278f96912779017988",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4ce71846fdcc4524ae55b33671370e81",
      "value": "‚Äá380M/380M‚Äá[00:02&lt;00:00,‚Äá167MB/s]"
     }
    },
    "abc8a5b72f034606b3fe3b9e5d0ef8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aea70cfcdeb94841bc1bc616cbbd1530": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b63361a7078549919d77d03dbe797aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99248a3e649b4b4fb4a6693fc781a3ae",
       "IPY_MODEL_f77ea67f2d644e949477239def77cce3",
       "IPY_MODEL_d5b9e59652574ae39463a62e91acbc6e"
      ],
      "layout": "IPY_MODEL_ef93f9eb83cb461f90c8b5cb85fab024"
     }
    },
    "ba121eca7b8f47559d54e7a3e02cc32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c85639bd3d5240278f96912779017988": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5b9e59652574ae39463a62e91acbc6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abc8a5b72f034606b3fe3b9e5d0ef8bb",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1b77a27420aa4d7d90af158073b07e0c",
      "value": "‚Äá1.84k/?‚Äá[00:00&lt;00:00,‚Äá33.1kB/s]"
     }
    },
    "d90a3d5595744e57b456a64dce5a6662": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbfe5751c7a843cbac2f760232c268b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bcc87bdc1de4542be9816c23ef918d8",
       "IPY_MODEL_789e1107230e4431a3b8fc36ccc0f370",
       "IPY_MODEL_3719dfed7c0840e6bc152595a3c85487"
      ],
      "layout": "IPY_MODEL_5d5dc262dd2a4ec282a1125b9dbaf63a"
     }
    },
    "ddf902c6d15240f294215ca9ebcec5b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e27a9962ac30459c8dae3d495c40ab8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef93f9eb83cb461f90c8b5cb85fab024": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4f3e6103c844b89b42c71709ee180ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f77ea67f2d644e949477239def77cce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1622a1fc464742bdaeeec936fb81fd2c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f4f3e6103c844b89b42c71709ee180ba",
      "value": 1
     }
    },
    "f8f99acab78d454b8505aa41c5b2fdb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbdc89bc7bca4e7b9af17480f1b51770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffb9e0b266b04194bb777818b49b7b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
